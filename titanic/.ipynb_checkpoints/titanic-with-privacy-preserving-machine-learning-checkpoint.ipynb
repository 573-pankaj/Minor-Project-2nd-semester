{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> üëãüõ≥Ô∏è Hello here from the Machine Learning team at <a href=\"https://github.com/zama-ai/concrete-ml\">Zama</a></h1>\n",
    "\n",
    "We're happy to submit our participation in this Kaggle competition. Our main idea was not only to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù but also to do it on encrypted data. This was possible thanks to our Python package: <a href=\"https://github.com/zama-ai/concrete-ml\">Concrete-ML</a> that aims to simplify the use of fully homomorphic encryption (FHE) for data scientists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Machine Learning on Titanic\n",
    "\n",
    "This notebook introduces a Privacy-Preserving Machine Learning (PPML) solution to the [Kaggle Titanic competition](https://www.kaggle.com/c/titanic/) using the [Concrete-ML](https://docs.zama.ai/concrete-ml) open-source framework. Its main ambition is to show that [Fully Homomorphic Encryption](https://en.wikipedia.org/wiki/Homomorphic_encryption) (FHE) can be used for protecting data when using a Machine Learning model to predict outcomes without degrading its performance. In this example, a [XGBoost](https://xgboost.readthedocs.io/en/stable/) classifier model will be considered as it achieves near state-of-the-art accuracy.\n",
    "\n",
    "With inspiration from the [ppxgboost repository](https://github.com/awslabs/privacy-preserving-xgboost-inference/blob/master/example/Titanic.ipynb), which is \"Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. SPDX-License-Identifier: Apache-2.0\".\n",
    "\n",
    "It also took some ideas from several upvoted public notebooks, including [this one](https://www.kaggle.com/code/startupsci/titanic-data-science-solutions/notebook) from Manav Sehgal and [this one](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy#Step-3:-Prepare-Data-for-Consumption) from LD Freeman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'concrete'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24860/834662671.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mConcreteXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'concrete'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from concrete.ml.sklearn import XGBClassifier as ConcreteXGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to run `make download_datasets` to have local versions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets\n",
    "if not Path(\"./local_datasets/titanic/train.csv\").is_file():\n",
    "    raise ValueError(\"Please download the datasets by running `make download_datasets`.\")\n",
    "\n",
    "train_data = pd.read_csv(\"./local_datasets/titanic/train.csv\")\n",
    "test_data = pd.read_csv(\"./local_datasets/titanic/test.csv\")\n",
    "datasets = [train_data, test_data]\n",
    "\n",
    "# Save the passenger IDs used for submission\n",
    "test_ids = test_data[\"PassengerId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   \t| PassengerId \t| Survived \t| Pclass \t|                                              Name \t|    Sex \t|  Age \t| SibSp \t| Parch \t|           Ticket \t|    Fare \t| Cabin \t| Embarked \t|\n",
    "|--:\t|------------:\t|---------:\t|-------:\t|--------------------------------------------------:\t|-------:\t|-----:\t|------:\t|------:\t|-----------------:\t|--------:\t|------:\t|---------:\t|\n",
    "| 0 \t|           1 \t|        0 \t|      3 \t|                           Braund, Mr. Owen Harris \t|   male \t| 22.0 \t|     1 \t|     0 \t|        A/5 21171 \t|  7.2500 \t|   NaN \t|        S \t|\n",
    "| 1 \t|           2 \t|        1 \t|      1 \t| Cumings, Mrs. John Bradley (Florence Briggs Th... \t| female \t| 38.0 \t|     1 \t|     0 \t|         PC 17599 \t| 71.2833 \t|   C85 \t|        C \t|\n",
    "| 2 \t|           3 \t|        1 \t|      3 \t|                            Heikkinen, Miss. Laina \t| female \t| 26.0 \t|     0 \t|     0 \t| STON/O2. 3101282 \t|  7.9250 \t|   NaN \t|        S \t|\n",
    "| 3 \t|           4 \t|        1 \t|      1 \t|      Futrelle, Mrs. Jacques Heath (Lily May Peel) \t| female \t| 35.0 \t|     1 \t|     0 \t|           113803 \t| 53.1000 \t|  C123 \t|        S \t|\n",
    "| 4 \t|           5 \t|        0 \t|      3 \t|                          Allen, Mr. William Henry \t|   male \t| 35.0 \t|     0 \t|     0 \t|           373450 \t|  8.0500 \t|   NaN \t|        S \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe:\n",
    "- the target variable: Survived\n",
    "- some numerical variables: PassengerID, Pclass, SbSp, Parch, Fare\n",
    "- some categorical (non-numerical) variables: Name, Sex, Ticket, Cabin, Embarked\n",
    "\n",
    "First, it seems that PassengerId and Ticket are supposed to be random Ids that should not impact the predictions so we can already remove them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column = [\"PassengerId\", \"Ticket\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Then, we can notice that some values are missing for the Cabin variable. We must therefore investigate a bit more about this by printing the total amounts of missing values for each variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 3, \"Train data\", \"-\" * 3)\n",
    "print(train_data.isnull().sum(), \"\\n\")\n",
    "print(\"-\" * 3, \"Test data\", \"-\" * 3)\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Training Data \t|     \t|\n",
    "|---------------\t|-----\t|\n",
    "| PassengerId   \t| 0   \t|\n",
    "| Survived      \t| 0   \t|\n",
    "| Pclass        \t| 0   \t|\n",
    "| Name          \t| 0   \t|\n",
    "| Sex           \t| 0   \t|\n",
    "| Age           \t| 177 \t|\n",
    "| SibSp         \t| 0   \t|\n",
    "| Parch         \t| 0   \t|\n",
    "| Ticket        \t| 0   \t|\n",
    "| Fare          \t| 0   \t|\n",
    "| Cabin         \t| 687 \t|\n",
    "| Embarked      \t| 2   \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Test Data     \t|     \t|\n",
    "|---------------\t|-----\t|\n",
    "| PassengerId   \t| 0   \t|\n",
    "| Pclass        \t| 0   \t|\n",
    "| Name          \t| 0   \t|\n",
    "| Sex           \t| 0   \t|\n",
    "| Age           \t| 86  \t|\n",
    "| SibSp         \t| 0   \t|\n",
    "| Parch         \t| 0   \t|\n",
    "| Ticket        \t| 0   \t|\n",
    "| Fare          \t| 1   \t|\n",
    "| Cabin         \t| 327 \t|\n",
    "| Embarked      \t| 0   \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for incomp_var in train_data.columns:\n",
    "    missing_val = pd.concat(datasets)[incomp_var].isnull().sum()\n",
    "    if missing_val > 0 and incomp_var != \"Survived\":\n",
    "        total_val = pd.concat(datasets).shape[0]\n",
    "        print(f\"Percentage of missing values in {incomp_var}: {missing_val/total_val*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of missing values in Age: 20.1%\n",
    "\n",
    "Percentage of missing values in Fare: 0.1%\n",
    "\n",
    "Percentage of missing values in Cabin: 77.5%\n",
    "\n",
    "Percentage of missing values in Embarked: 0.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only four variables are incomplete: Cabin, Age, Embarked and Fare. However, the Cabin one seems to be missing quite more data than the others:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Cabin variable misses more than 2/3 of its values, it might not be relevant to keep it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column += [\"Cabin\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Remove irrelevant variables\n",
    "    dataset.drop(drop_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other ones, we can replace the missing values using:\n",
    "- the median value for Age and Fare\n",
    "- the most common value for Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    # Complete missing Age values with median\n",
    "    dataset.Age.fillna(dataset.Age.median(), inplace=True)\n",
    "\n",
    "    # Complete missing Embarked values with the most common one\n",
    "    dataset.Embarked.fillna(dataset.Embarked.mode()[0], inplace=True)\n",
    "\n",
    "    # Complete missing Fare values with median\n",
    "    dataset.Fare.fillna(dataset.Fare.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "We can manually extract and create new features in order to help the model interpret some behaviors and correctly predict an outcome. Those new features are:\n",
    "- FamilySize: The size of the family the individual was traveling with, with 1 being someone that traveled alone. \n",
    "- IsAlone: A boolean variable stating if the individual was traveling alone (1) or not (0). This might help the model to emphasize on this idea of traveling with relatives or not.\n",
    "- Title: The individual's title (Mr, Mrs, ...), often indicating a certain social status.\n",
    "- Farebin and AgeBin: Binned version of the Fare and Age variables. It groups values together, generally reducing the impact of minor observation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_labels(bin_name, number_of_bins):\n",
    "    labels = []\n",
    "    for i in range(number_of_bins):\n",
    "        labels.append(bin_name + f\"_{i}\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Emphasize on relatives\n",
    "    dataset[\"FamilySize\"] = dataset.SibSp + dataset.Parch + 1\n",
    "\n",
    "    dataset[\"IsAlone\"] = 1\n",
    "    dataset.IsAlone[dataset.FamilySize > 1] = 0\n",
    "\n",
    "    # Consider an individual's social status\n",
    "    dataset[\"Title\"] = dataset.Name.str.extract(r\" ([A-Za-z]+)\\.\", expand=False)\n",
    "\n",
    "    # Group fares and ages in \"bins\" or \"buckets\"\n",
    "    dataset[\"FareBin\"] = pd.qcut(dataset.Fare, 4, labels=get_bin_labels(\"FareBin\", 4))\n",
    "    dataset[\"AgeBin\"] = pd.cut(dataset.Age.astype(int), 5, labels=get_bin_labels(\"AgeBin\", 5))\n",
    "\n",
    "    # Remove now-irrelevant variables\n",
    "    drop_column = [\"Name\", \"SibSp\", \"Parch\", \"Fare\", \"Age\"]\n",
    "    dataset.drop(drop_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on the titles' distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(datasets)\n",
    "titles = data.Title.value_counts()\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Titles  \t| Counts  \t|\n",
    "|----------\t|-----\t|\n",
    "| Mr       \t| 757 \t|\n",
    "| Miss     \t| 260 \t|\n",
    "| Mrs      \t| 197 \t|\n",
    "| Master   \t| 61  \t|\n",
    "| Rev      \t| 8   \t|\n",
    "| Dr       \t| 8   \t|\n",
    "| Col      \t| 4   \t|\n",
    "| Mlle     \t| 2   \t|\n",
    "| Major    \t| 2   \t|\n",
    "| Ms       \t| 2   \t|\n",
    "| Lady     \t| 1   \t|\n",
    "| Sir      \t| 1   \t|\n",
    "| Mme      \t| 1   \t|\n",
    "| Don      \t| 1   \t|\n",
    "| Capt     \t| 1   \t|\n",
    "| Countess \t| 1   \t|\n",
    "| Jonkheer \t| 1   \t|\n",
    "| Dona     \t| 1   \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly observe that only a few titles represent most of the individuals. In order to prevent the model from becoming overly specific, we decide to group all the \"uncommon\" titles together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_titles = titles[titles < 10].index\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset.Title.replace(uncommon_titles, \"Rare\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummification\n",
    "\n",
    "Finally, we can \"dummify\" the remaining categorical variables. Dummification is a common technique of transforming categorical (non-numerical) data into numerical data without having to map values or consider any order between each of them. The idea is to take all the different values found in a variable and create a new associated binary variable. \n",
    "\n",
    "For example, the \"Embarked\" variable has three categorical values: \"S\", \"C\" and \"Q\". Dummifying the data will create three new variables, \"Embarked_S\", \"Embarked_C\" and \"Embarked_Q\", and set the value of \"Embarked_S\" (resp. \"Embarked_C\" and \"Embarked_Q\") to 1 for each data point initially labeled with \"S\" (resp. \"C\" and \"Q\") in the \"Embarked\" variable, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = train_data.select_dtypes(exclude=np.number).columns\n",
    "x_train = pd.get_dummies(train_data, prefix=categorical_features)\n",
    "x_test = pd.get_dummies(test_data, prefix=categorical_features).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the target variable from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Survived\"\n",
    "x_train = x_train.drop(columns=[target])\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = train_data[target].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "### Training with XGBoost\n",
    "\n",
    "Let's first train a classifier model using XGBoost. Since several parameters have to be fixed beforehand, we use scikit-learn's GridSearchCV method to perform cross validation in order to maximize our chance to find the best ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Cross-Validation generator\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "\n",
    "# Set the parameters to tune.\n",
    "# Those ranges are voluntarily small in order to keep the FHE execution time per inference\n",
    "# relatively low. In fact, we found out that, in this particular Titanic example, models with\n",
    "# larger numbers of estimators or maximum depth don't score a much better accuracy.\n",
    "param_grid = {\n",
    "    \"max_depth\": list(range(1, 5)),\n",
    "    \"n_estimators\": list(range(1, 5)),\n",
    "    \"learning_rate\": [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Instantiate and fit the model through grid-search cross-validation\n",
    "time_begin = time.time()\n",
    "model = GridSearchCV(XGBClassifier(), param_grid, cv=cv, scoring=\"roc_auc\")\n",
    "model.fit(x_train, y_train)\n",
    "cv_xgb_duration = time.time() - time_begin\n",
    "\n",
    "print(f\"Best hyperparameters found in {cv_xgb_duration:.2f}s :\", model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters found in 4.40s : {'learning_rate': 1, 'max_depth': 4, 'n_estimators': 4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Concrete-ML\n",
    "\n",
    "Now, let's do the same with Concrete-ML's XGBClassifier method. \n",
    "\n",
    "In order to do so, we need to specify the number of bits over which inputs, outputs and weights will be quantized. This value can influence the precision of the model as well as its inference running time, and therefore can lead the grid-search cross-validation to find a different set of parameters. In our case, setting this value to 2 bits outputs an excellent accuracy score while running faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Concrete-ML model needs an additional parameter used for quantization\n",
    "param_grid[\"n_bits\"] = [2]\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "\n",
    "# Instantiate and fit the model through grid-search cross-validation\n",
    "time_begin = time.time()\n",
    "concrete_model = GridSearchCV(ConcreteXGBClassifier(), param_grid, cv=cv, scoring=\"roc_auc\")\n",
    "concrete_model.fit(x_train, y_train)\n",
    "cv_concrete_duration = time.time() - time_begin\n",
    "\n",
    "print(f\"Best hyperparameters found in {cv_concrete_duration:.2f}s :\", concrete_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters found in 32.79s : {'learning_rate': 0.1, 'max_depth': 4, 'n_bits': 2, 'n_estimators': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the predictions in FHE on the complete test set of 418 data points using the above hyperparameters can take up to 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predictions in clear using XGBoost\n",
    "clear_predictions = model.predict(x_test)\n",
    "\n",
    "# Compute the predictions in clear using Concrete-ML\n",
    "clear_quantized_predictions = concrete_model.predict(x_test)\n",
    "\n",
    "# Compile the Concrete-ML model on a subset\n",
    "concrete_model.best_estimator_.compile(x_train[:100])\n",
    "\n",
    "# Generate the keys\n",
    "# This step is not necessary, keygen() is called directly within the predict method. However, since\n",
    "# the keys are stored in cache by default, it is useful to run it beforehand in order to be able to\n",
    "# measure the prediction executing time separately from the key generation one\n",
    "time_begin = time.time()\n",
    "concrete_model.best_estimator_.fhe_tree.keygen()\n",
    "key_generation_duration = time.time() - time_begin\n",
    "\n",
    "# Compute the predictions in FHE using Concrete-ML\n",
    "# Giving x_test as an input to the predict method is possible in Concrete-ML but without the use of\n",
    "# batches. We therefore decided to directly loop over it in order to better visualize the progress\n",
    "# using the tqdm package, as it doesn't impact the overall execution time.\n",
    "fhe_predictions = []\n",
    "time_begin = time.time()\n",
    "for data_point in tqdm(x_test):\n",
    "    fhe_predictions.append(\n",
    "        concrete_model.best_estimator_.predict(np.array([data_point]), execute_in_fhe=True)[0]\n",
    "    )\n",
    "prediction_duration = time.time() - time_begin\n",
    "\n",
    "print(f\"Key generation time: {key_generation_duration:.2f}s\")\n",
    "print(f\"Total execution time for {len(clear_predictions)} inferences: {prediction_duration:.2f}s\")\n",
    "print(f\"Execution time per inference in FHE : {prediction_duration / len(clear_predictions):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 418/418 [18:58<00:00,  2.72s/it]\n",
    "\n",
    "Key generation time: 12.00s\n",
    "\n",
    "Total execution time for 418 inferences: 1138.80s\n",
    "\n",
    "Execution time per inference in FHE : 2.72s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FHE computations are expected to be exact. This means that the model executed in FHE results in the same predictions as the Concrete-ML one, which is executed in clear and only considers quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_equal_preds = np.sum(fhe_predictions == clear_quantized_predictions)\n",
    "pred_similarity = number_of_equal_preds / len(clear_predictions) * 100\n",
    "print(\n",
    "    \"Prediction similarity between both Concrete-ML models (quantized clear and FHE): \"\n",
    "    f\"{pred_similarity:.2f}%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction similarity between both Concrete-ML models (quantized clear and FHE): 100.00%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as seen previously, the grid-search cross-validation was done separately between the XGBoost model and the Concrete-ML one. For this reason, the two models do not share the same set of hyperparameters, making their decision boundaries different.\n",
    "\n",
    "Comparing how similar their predictions are one by one is thus irrelevant and only the final accuracy score given by the Kaggle platform should be considered to assess their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Kaggle Submission File\n",
    "\n",
    "When [submitted](https://www.kaggle.com/competitions/titanic/submit) to the Kaggle platform, the FHE model outputs an accuracy of 78% ([leaderboard](https://www.kaggle.com/competitions/titanic/leaderboard?search=concrete)). In comparison, the XGBoost clear one also scores around 77%.\n",
    "\n",
    "In fact, using the given dataset, most of the submitted notebooks do not seem to exceed 79% of accuracy. Therefore, additional pre-processing and feature engineering might help increase our current score but probably not by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the FHE predictions\n",
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        \"PassengerId\": test_ids,\n",
    "        \"Survived\": fhe_predictions,\n",
    "    }\n",
    ")\n",
    "submission.to_csv(\"titanic_submission_fhe.csv\", index=False)\n",
    "\n",
    "# Save the XGBoost clear predictions\n",
    "submission = pd.DataFrame(\n",
    "    {\n",
    "        \"PassengerId\": test_ids,\n",
    "        \"Survived\": clear_predictions,\n",
    "    }\n",
    ")\n",
    "submission.to_csv(\"titanic_submission_xgb_clear.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "If you liked this work, please have a look to our repo: **we're open source!**\n",
    "<br/>\n",
    "‚≠êÔ∏è Star us on Github here: <a href=\"https://github.com/zama-ai/concrete-ml\" target=\"_blank\">github.com/zama-ai/concrete-ml</a>\n",
    "<br/>\n",
    "üëã And ask us anything on our community forum: <a href=\"https://community.zama.ai/c/concrete-ml/8?utm_source=participation&utm_medium=titanic&utm_campaign=kaggle\" target=\"_blank\">community.zama.ai</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
